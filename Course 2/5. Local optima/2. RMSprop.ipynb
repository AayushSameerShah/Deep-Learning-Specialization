{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fd8c56-2112-4002-abaa-4330b5417fe8",
   "metadata": {},
   "source": [
    "# RMSprop *(Big guns!)*\n",
    "\n",
    "> **Helps to \"reduce high change\" in several weights -- for faster and smoother learning.**\n",
    "\n",
    "- It is the **first adaptive learning rate** algorithm!\n",
    "- In here, the learning rate adapts to each weights!\n",
    "- **You actually achieve** the \"<u>low moment in the vertical and high in horizontal direction</u>\" thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c81795-9353-4790-b145-ff028f234ae8",
   "metadata": {},
   "source": [
    "## Let's the difference lies in \"HOW WE UPDATE\"\n",
    "\n",
    "Let's see how the momentum works.\n",
    "\n",
    "**MOMENTUM**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_{dW} &= \\beta v_{dW} + (1 - \\beta) dW \\\\\n",
    "v_{db} &= \\beta v_{db} + (1 - \\beta) db \\\\\n",
    "W &= W - \\alpha v_{dW} \\\\\n",
    "b &= b - \\alpha v_{db}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Easy right?\n",
    "- Just the EWMA learning\n",
    "- And then applying.\n",
    "\n",
    "---\n",
    "\n",
    "**RMSprop**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S_{dW} &= \\beta S_{dW} + (1 - \\beta) (dW)^2 \\\\\n",
    "S_{db} &= \\beta S_{db} + (1 - \\beta) (db)^2 \\\\\n",
    "W &= W - \\alpha \\frac{dW}{\\sqrt{S_{dW}}} \\\\\n",
    "b &= b - \\alpha \\frac{db}{\\sqrt{S_{db}}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- The higher gradients will **get even higher** thus their \"storage\" the EWMA will be usually high.\n",
    "- That means when you **divide later while update** the **whole term becomes smaller**.\n",
    "- Which makes the small updates in the irrelevant direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d970c-23c1-4a85-a85c-5ac8fc0e875c",
   "metadata": {},
   "source": [
    "## GD-Momentum\n",
    "\n",
    "<img src=\"./images/gd-mom.png\">\n",
    "\n",
    "## RMSprop\n",
    "\n",
    "<img src=\"./images/rmsprop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87144f2f-2f4b-4fd3-a6f1-7c89da3306a7",
   "metadata": {},
   "source": [
    "üö® Problem before RMSProp: **Uneven, fucked-up learning**\n",
    "\n",
    "- Imagine your loss surface looks like this:\n",
    "\t- Super steep in one direction\n",
    "    - Super flat in another!\n",
    "\n",
    "That's what happens when features have different scales. Like one weight wants to explode, another barely moves.\n",
    "\n",
    "üß† Enter RMSProp: **The Smooth Criminal**\n",
    "\n",
    "RMSProp is like:\n",
    "> ‚ÄúBro chill, let me SLOW YOU DOWN in steep directions and let you move freely in flatter ones.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f3572-aafe-46f3-a9f0-26166aa14c33",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "\n",
    "### Some optional notes for the names of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298ae73-b1b6-4413-867c-b46ce4a8bec4",
   "metadata": {},
   "source": [
    "The terms \"`v`\" in momentum and \"`S`\" in RMSProp are used because they each describe the role of these stored variables in the optimization step.\n",
    "\n",
    "#### Why Is It Called \"`v`\" in Momentum?\n",
    "\n",
    "In gradient descent with momentum, the stored variable is denoted by $v$ because it stands for \"velocity\".\n",
    "- Momentum is inspired by physics: just as velocity represents accumulated movement in a direction due to past force, $v$ in momentum tracks the running total of gradients.\n",
    "- This term helps the algorithm accelerate in consistent directions and smooths out updates, reducing oscillations and speeding up convergence.\n",
    "\n",
    "#### Why Is It Called \"`S`\" in RMSProp?\n",
    "\n",
    "In RMSProp, the stored variable is denoted by $S$ because it stands for \"squared\" or the running sum of squared gradients.\n",
    "- RMSProp keeps a moving average of the squared gradients of each parameter; hence, the notation $ S_{dW} $ or $ S_{db} $ is descriptive.\n",
    "- This \"S\" helps scale the parameter updates by their historical magnitude, allowing the optimizer to normalize learning rates in each direction.\n",
    "\n",
    "#### Summary Table\n",
    "\n",
    "| Algorithm        | Storage Symbol | Meaning                         | Analogy                                                      |\n",
    "|------------------|---------------|----------------------------------|--------------------------------------------------------------|\n",
    "| Momentum         | $$ v $$       | Velocity (running gradient sum)  | How a rolling ball keeps gaining motion (past + present)     |\n",
    "| RMSProp          | $$ S $$       | Squared (running sum of squares) | Tracking average speed squared for a car's engine tuning     |\n",
    "\n",
    "These choices make the math clear and connect to the intuition behind each optimization method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
