{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca084c0b-61e6-4e3e-b6ed-c6cf2382045b",
   "metadata": {},
   "source": [
    "# The regularization...\n",
    "\n",
    "Let's consider the main hero `lambda`. That controls everything. \n",
    "- Higher it is, tigher the regularization will be.\n",
    "- Lower it is, no regularization there will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563488e-f368-4d77-8c4a-f134e59910f2",
   "metadata": {},
   "source": [
    "<img src=\"./images/why.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d22f1-a7e1-4ee0-925d-b977ab158a53",
   "metadata": {},
   "source": [
    "**WHY-1**:\n",
    "- Higher lambda penalized the weights `w` and pushes them towards `0`.\n",
    "- The most of the weights in the network are zero (close to zero).\n",
    "- Thus the network is no longer learning non-linear patterns and is acting as a **simple logistic regression** model.\n",
    "- The network is hella simplified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708908c-c3e2-491c-bf8f-dd1da98d3072",
   "metadata": {},
   "source": [
    "<img src=\"./images/why-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda4bf3-9700-45ba-ba7d-e575b61485cd",
   "metadata": {},
   "source": [
    "**WHY-2**:\n",
    "- Consider the model is made of **tanh**.\n",
    "- Higher lambda makes weights to be in the **smaller range**\n",
    "- The smaller range in the tanh translates to the linear portion of values.\n",
    "- Thus it will make the **overall model as a linear regression model**\n",
    "- Which is just like using the `linear` as the activation -- instead of tanh.\n",
    "- No non-linearity learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f237964-250d-40e4-9deb-92205ee60e8d",
   "metadata": {},
   "source": [
    "1. Regularization\n",
    "    - Decay (L1, L2)\n",
    "    - Dropout\n",
    "        - Inverted dropout: It will divide the result after activation by the dropout probability to keep things balanced. Nothing done in the test time! (use this)\n",
    "        - Vanilla dropout: No division at train time, but at test time, multiply! (less recommended)\n",
    "2. Add more data\n",
    "    - Gather\n",
    "    - Create (augment)\n",
    "3. Early Stopping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b37696-6033-4089-bf14-b431e32a3dd5",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "- Helps the stable training\n",
    "- The weights get same treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079ba60-9f20-4169-bebf-492308692f1a",
   "metadata": {},
   "source": [
    "# Exploding/Vanishing Gradients\n",
    "- Try initializing the weights with some cool initialization method like \"Xavier\" or \"He\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c1bde-6e07-4ed1-9cec-e7db0183d864",
   "metadata": {},
   "source": [
    "<img src=\"./images/everything.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
